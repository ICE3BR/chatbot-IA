### ğŸ“Œ **ICEBERG IA - Chatbot Inteligente com DeepSeek-R1**
**ICEBERG IA** Ã© um chatbot interativo baseado em **modelos de IA avanÃ§ados** e executado na **Ollama**, utilizando o modelo **DeepSeek-R1:8b** para processamento eficiente e inteligente de linguagem natural.

ğŸš€ **Destaques do projeto:**
- Modelo de IA **DeepSeek-R1:8b**, altamente otimizado para desempenho superior em **raciocÃ­nio, cÃ³digo e matemÃ¡tica**.
- PersistÃªncia de **histÃ³rico de conversa** entre interaÃ§Ãµes.
- Suporte para **comandos interativos** no terminal.
- Interface otimizada para **uso eficiente de memÃ³ria e processamento**.

---

## ğŸ“¦ **InstalaÃ§Ã£o**
### 1ï¸âƒ£ **PrÃ©-requisitos**
Antes de iniciar, certifique-se de ter os seguintes requisitos instalados:

- **Python** (>= 3.8)
- **Ollama** ([instalar aqui](https://ollama.ai/))
- **Gerenciador de dependÃªncias**: [UV](https://docs.astral.sh/uv/)
- **DependÃªncias Python** (listadas no `pyproject.toml`)

### 2ï¸âƒ£ **Baixar e instalar dependÃªncias**
Clone o repositÃ³rio e instale as dependÃªncias:

```bash
git clone https://github.com/ICE3BR/chatbot-IA.git
cd chatbot-ia
uv sync
```

> âš ï¸ **Nota:** Se estiver utilizando **WSL2** ou **Ubuntu**, garanta que a **Ollama** esteja configurada corretamente.

---

## ğŸš€ **Como executar**
ApÃ³s instalar tudo, execute o chatbot com:

```bash
python main.py
```

ğŸ”¹ O terminal exibirÃ¡ a mensagem de boas-vindas:  
```bash
Seja bem-vindo ao ICEBERG IA (Modelo: deepseek-r1:8b)
Digite 'sair' para encerrar e salvar o histÃ³rico do o chatbot.
```

ğŸ”¹ Agora, vocÃª pode interagir digitando suas perguntas e recebendo respostas contextuais.

---

## ğŸ–¥ï¸ **Requisitos de Hardware**
O desempenho do **ICEBERG IA** varia de acordo com o modelo escolhido. Modelos maiores exigem mais memÃ³ria e processamento para rodar de forma eficiente.

### ğŸ”¹ **Como escolher um modelo compatÃ­vel com seu hardware?**
Cada modelo de IA possui um **tamanho de memÃ³ria necessÃ¡rio** para ser carregado e processado corretamente. Como regra geral:

- **O tamanho do modelo indica a memÃ³ria mÃ­nima necessÃ¡ria** para carregÃ¡-lo na GPU ou na RAM.
- **Se sua GPU nÃ£o tiver VRAM suficiente, o processamento serÃ¡ feito na CPU**, o que pode resultar em respostas mais lentas.

| Modelo             | ParÃ¢metros | Tamanho do Arquivo | Requisitos de MemÃ³ria |
|--------------------|-----------|--------------------|----------------------|
| **DeepSeek-R1**    | 1.5B      | 2.7GB              | **3GB RAM/VRAM** |
| **DeepSeek-R1**    | 8B        | 4.9GB              | **5GB RAM/VRAM** |
| **DeepSeek-R1**    | 14B       | 7.5GB              | **8GB RAM/VRAM** |
| **DeepSeek-R1**    | 32B       | 16GB               | **16GB RAM/VRAM** |

ğŸ’¡ **Dica:**  
Se sua mÃ¡quina **nÃ£o possui GPU suficiente**, o modelo pode rodar via CPU, mas **o tempo de resposta serÃ¡ maior**.  
Se deseja **respostas mais rÃ¡pidas**, recomenda-se **executar o chatbot em uma GPU com VRAM suficiente** para o modelo escolhido.

> **Exemplo:** Se vocÃª escolher o **DeepSeek-R1:8B**, que possui **4.9GB de tamanho**, sua GPU precisa de **pelo menos 5GB de VRAM disponÃ­vel** para carregÃ¡-lo eficientemente.

---

## ğŸ¤– **Modelos de IA disponÃ­veis**
O **ICEBERG IA** suporta mÃºltiplos modelos na **Ollama**. Atualmente, recomenda-se o **DeepSeek-R1:8b**, mas vocÃª pode alternar para outros modelos no cÃ³digo, alterando a variÃ¡vel `IA_MODEL`.

| Modelo             | ParÃ¢metros | Tamanho  | Comando Ollama                        |
|--------------------|-----------|----------|---------------------------------------|
| **DeepSeek-R1**    | 8B        | 4.9GB    | `ollama run deepseek-r1:8b`          |
| **DeepSeek-R1**    | 1.5B      | 2.7GB    | `ollama run deepseek-r1:1.5b`        |
| **Llama 3.1**      | 8B        | 5.2GB    | `ollama run llama3.1:8b`             |
| **Llama 3.2**      | 3B        | 3.5GB    | `ollama run llama3.2:3b`             |

> ğŸ’¡ **RecomendaÃ§Ã£o:** O **DeepSeek-R1:8B** Ã© um modelo **mais eficiente e rÃ¡pido**, consumindo **menos recursos** e entregando um desempenho comparÃ¡vel ao **GPT-O1**.

### ğŸ”¹ **O que sÃ£o os parÃ¢metros?**
Os parÃ¢metros de um modelo representam os pesos treinÃ¡veis, indicando sua capacidade de raciocÃ­nio. Quanto maior o nÃºmero de parÃ¢metros, mais detalhadas e precisas sÃ£o as respostas, mas tambÃ©m maior Ã© o uso de memÃ³ria e processamento. Modelos menores sÃ£o mais leves, enquanto os maiores oferecem maior desempenho em tarefas complexas. 

---

## ğŸ’¾ **HistÃ³rico de Conversa**
O chatbot mantÃ©m um **histÃ³rico de interaÃ§Ãµes** no arquivo `historico_conversa.txt`, permitindo a continuidade do diÃ¡logo entre sessÃµes.

ğŸ”¹ **Formato do histÃ³rico salvo:**
```
[28-01-2025 15:30:45] | VocÃª: OlÃ¡, como estÃ¡?
[28-01-2025 15:30:47] Bot: OlÃ¡! Estou bem. Como posso te ajudar?
```

> ğŸ” **ConfiguraÃ§Ã£o:** O histÃ³rico Ã© limitado a **200 interaÃ§Ãµes** por padrÃ£o (`MAX_INTERACOES = 200`), mas pode ser alterado no cÃ³digo.

---

## ğŸ› ï¸ **PersonalizaÃ§Ãµes**
VocÃª pode modificar o comportamento do chatbot alterando o arquivo `main.py`:

### ğŸ“Œ **Alterar modelo de IA**
Edite a variÃ¡vel `IA_MODEL` no cÃ³digo:

```python
IA_MODEL = "deepseek-r1:8b"  # Altere aqui para outro modelo
```

### ğŸ“Œ **Alterar limite de histÃ³rico**
Modifique a constante `MAX_INTERACOES` para ajustar o nÃºmero de mensagens armazenadas.

```python
MAX_INTERACOES = 100  # Define um novo limite
```

---

## ğŸ“œ **LicenÃ§a**
Este projeto Ã© distribuÃ­do sob a **MIT License**. O modelo **DeepSeek-R1** tambÃ©m Ã© licenciado sob **MIT**, permitindo uso comercial e modificaÃ§Ãµes.

ğŸ”— **Mais informaÃ§Ãµes sobre DeepSeek-R1:**  
[DeepSeek R1 no Ollama](https://ollama.ai/library/deepseek-r1)
